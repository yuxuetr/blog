[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/convert_latex_to_pptx/index.html",
    "href": "posts/convert_latex_to_pptx/index.html",
    "title": "从LaTex到PowerPoint",
    "section": "",
    "text": "xeLaTex         ImageMagick\nBeamer -----------&gt; PDF --------------&gt; PowerPoint\n\n\n\n\n\n\\documentclass{beamer} % 声明Latex文档格式为幻灯片\n\\begin{document} % 要显示的内容都必须卸载document环境中\n  % frame环境表示单张幻灯片页面\n  \\section{幻灯片章节名} % 显示在目录页面中的条目\n  \\subsection{子章节名} % 可选\n  \\begin{frame}{幻灯片主题}{幻灯片子主题}\n  \\end{frame}\n\\end{document}\n\n\n\n\nctex: 支持中文(UTF8字符)\ncolor/xcolor: 颜色\nfontspec: 字体\ngraphicx: 图形包\ntikz: 图形包\ntikz-uml: 基于tikz的UML包\n\n\n\n\n\n首页的设置放在\\begin{document}之前，不在正文部分\n\n\\title{主题}\n\\subtitle{子主题}\n\\author{作者}\n\\institute{研究所}\n\\date{日期}\n\n显示首页要放在document环境中\n\n\\begin{frame}\n  \\titlepage\n\\end{frame}\n\n\n\n\\begin{frame}\n  \\frametitle{目录}\n  \\tableofcontents\n\\end{frame}\n\n\n\n\nLists\nPictures\nDescription\nTables\nBlocks\nCode\nHyperlinks\nTheorem\n\n\n\n\n有序列表: enumerate\n无序列表: itemize\n\n\n\n\\begin{itemize}\n  \\item 条目1\n  \\item 条目2\n\\end{itemize}\n\n\n\n\n不同的序号表示方式:\n\n\n数值\n字母\n罗马数字\ni\n\n% 基于数字，默认方式\n\\begin{enumerate}\n  \\item 条目1\n  \\item 条目2\n\\end{enumerate}\n\n\\begin{enumerate}[I]\n  \\item 条目1\n  \\item 条目2\n\\end{enumerate}\n\n\\begin{enumerate}[i]\n  \\item 条目1\n  \\item 条目2\n\\end{enumerate}\n\n\\begin{enumerate}[(I)]\n  \\item 条目1\n  \\item 条目2\n\\end{enumerate}\n\n给序号添加上括号\n\n\\begin{enumerate}[(I)]\n  \\item 条目1\n  \\item 条目2\n\\end{enumerate}\n\n给需要加上强调内容\n\n\n\n\n\n导入图片，格式包括: jpg, png, pdf\n\\begin{figure}\n  % 导入图片指令, scale为图片缩放系数\n  \\includegraphics[scale=0.5]{文件路径}\n  \\caption{图片说明}\n\\end{figure}\n\n\n\n是一种Lists环境，是要用在\\item后面，内容放在方括号里面\n\\begin{description}\n  \\item[API] Application Programing Interface\n  \\item[LAN] Local Area Network\n\\end{description}\n\n\n\n\n\\hline: 表示表格中的水平线，横线\n{l | c | r}\n\n|: 用来表示表格中竖线\nl: 表示左对齐\nc: 居中\nr: 表示右对齐\n\n&: 单行中实际内容每个列之间的隔离符号\n\\\\: 表示行的换行\n\n\n\\begin{table}\n  \\begin{tabular}{l | c | r} % 表示列，如图表示三个列的竖线由 | 表示, l表示左对齐\n    \\hline % 表示画一条表格的横线\n  \\end{tabular}\n\\end{table}\n\n\n\n以块的方式显示信息。普通块与警告块。\n\\begin{block}{块名称}\n   显示内容\n\\end{block}\n\n\\begin{alertblock}{块名称} % 显示未红色\n   显示内容\n\\end{alertblock}\n\n\n\n使用listings软件包 + 设置代码风格 - 设置字体 - 设置代码字体颜色 - 设置代码风格 - 应用代码风格 + 实际代码环境\n\n设置字体，颜色以及代码风格并全局应用\n\n\\setmonofont{Monaco} % 设置等宽字体为Monaco\n\n% 设置颜色\n\\definecolor{codegreen}{rgb}{0, 0.6, 0}\n\\definecolor{codegray}{rgb}{0.5, 0.5, 0.5}\n\\definecolor{codepurple}{rgb}{0.58, 0, 0.82}\n\\definecolor{backcolor}{rgb}{0.95, 0.95, 0.92}\n\n% 代码风格设置\n\\lstdefinestyle{customstyle}{\n    backgroundcolor=\\color{backcolor},\n    commentstyle=\\color{codegreen},\n    keywordstyle=\\color{magenta},\n    numberstyle=\\color{codegray},\n    stringstyle=\\color{codepurple},\n    basicstyle=\\ttfamily\\footnotesize,\n    breakatwhitespace=false,\n    breaklines=true,\n    captionpos=b,\n    keepspaces=true,\n    numbers=left,\n    numbersep=3pt,\n    showspaces=flase,\n    showstringspaces=false,\n    showtabs=false,\n    tabsize=2,\n}\n% 应用代码风格\n\\lstset{style=customstyle}\n\n应用代码风格 在frame环境中\n\n\\lstinputlisting[\n  firstline=1, % 从文件导入代码，从文件的开始的行号\n  lastline=5, % 从文件导入代码，从文件的结束的行号\n  language=Python, % 编程语言\n  frame=single,\n  caption=First ten lines of some Python code, % 代码的说明\n  label=python\n  ]{perceptron_and.py} % 导入代码的文件\n\n\n\n\\hyperlink{链接文字描述}{链接地址}\n\n\n\n定理和推论环境会自动将其内容放在斜体字中。 beamer会自动加载amsmath软件包，所以可以直接添加数学公式。\n\\begin{theorem}[定理名称]\n$ a^2 + b^2 = c^2$\n\\end{theorem}\n\\begin{corollary} % 推论\n$ x + y = y + x  $\n\\end{corollary}\n\\begin{proof} % 论据\n$\\omega +\\phi = \\epsilon $\n\\end{proof}\n\n\n\n\n\n把Logo作为一个图片或者PDF文件\n以插入图片的方式进行插入\n对相关属性进行设置\n\n\n\n\n\n居中\n换行\n\n\n\n\\centering % 写在需要居中的环境命令上面一行\n\n\n\n\\\\\n\n\n\n\n\n使用xelatex将*.tex生成PDF文件\n使用ImageMagick的脚本将PDF转成PowerPoint\n\nxelatex demo.tex # 生成demo.pdf\n# https://github.com/ashafaei/pdf2pptx/blob/master/pdf2pptx.sh\n./pdf2pptx.sh demo.pdf # 生成 demo.pptx\n\n\n\n\n                   Pandoc\nMarkdown/Orgmode -----------&gt;  PDF/epub\npandoc参数说明 - -f: 表示from, 表示源格式 - -t: 表示to, 表示目标格式 - -o: 表示输出文件名\npandoc -f org -t pdf -o demo.pdf demo.org\npandoc -f markdown -t pdf -o demo.pdf demo.md"
  },
  {
    "objectID": "posts/convert_latex_to_pptx/index.html#从latex-beamer到powerpoint",
    "href": "posts/convert_latex_to_pptx/index.html#从latex-beamer到powerpoint",
    "title": "从LaTex到PowerPoint",
    "section": "",
    "text": "xeLaTex         ImageMagick\nBeamer -----------&gt; PDF --------------&gt; PowerPoint\n\n\n\n\n\n\\documentclass{beamer} % 声明Latex文档格式为幻灯片\n\\begin{document} % 要显示的内容都必须卸载document环境中\n  % frame环境表示单张幻灯片页面\n  \\section{幻灯片章节名} % 显示在目录页面中的条目\n  \\subsection{子章节名} % 可选\n  \\begin{frame}{幻灯片主题}{幻灯片子主题}\n  \\end{frame}\n\\end{document}\n\n\n\n\nctex: 支持中文(UTF8字符)\ncolor/xcolor: 颜色\nfontspec: 字体\ngraphicx: 图形包\ntikz: 图形包\ntikz-uml: 基于tikz的UML包\n\n\n\n\n\n首页的设置放在\\begin{document}之前，不在正文部分\n\n\\title{主题}\n\\subtitle{子主题}\n\\author{作者}\n\\institute{研究所}\n\\date{日期}\n\n显示首页要放在document环境中\n\n\\begin{frame}\n  \\titlepage\n\\end{frame}\n\n\n\n\\begin{frame}\n  \\frametitle{目录}\n  \\tableofcontents\n\\end{frame}\n\n\n\n\nLists\nPictures\nDescription\nTables\nBlocks\nCode\nHyperlinks\nTheorem\n\n\n\n\n有序列表: enumerate\n无序列表: itemize\n\n\n\n\\begin{itemize}\n  \\item 条目1\n  \\item 条目2\n\\end{itemize}\n\n\n\n\n不同的序号表示方式:\n\n\n数值\n字母\n罗马数字\ni\n\n% 基于数字，默认方式\n\\begin{enumerate}\n  \\item 条目1\n  \\item 条目2\n\\end{enumerate}\n\n\\begin{enumerate}[I]\n  \\item 条目1\n  \\item 条目2\n\\end{enumerate}\n\n\\begin{enumerate}[i]\n  \\item 条目1\n  \\item 条目2\n\\end{enumerate}\n\n\\begin{enumerate}[(I)]\n  \\item 条目1\n  \\item 条目2\n\\end{enumerate}\n\n给序号添加上括号\n\n\\begin{enumerate}[(I)]\n  \\item 条目1\n  \\item 条目2\n\\end{enumerate}\n\n给需要加上强调内容\n\n\n\n\n\n导入图片，格式包括: jpg, png, pdf\n\\begin{figure}\n  % 导入图片指令, scale为图片缩放系数\n  \\includegraphics[scale=0.5]{文件路径}\n  \\caption{图片说明}\n\\end{figure}\n\n\n\n是一种Lists环境，是要用在\\item后面，内容放在方括号里面\n\\begin{description}\n  \\item[API] Application Programing Interface\n  \\item[LAN] Local Area Network\n\\end{description}\n\n\n\n\n\\hline: 表示表格中的水平线，横线\n{l | c | r}\n\n|: 用来表示表格中竖线\nl: 表示左对齐\nc: 居中\nr: 表示右对齐\n\n&: 单行中实际内容每个列之间的隔离符号\n\\\\: 表示行的换行\n\n\n\\begin{table}\n  \\begin{tabular}{l | c | r} % 表示列，如图表示三个列的竖线由 | 表示, l表示左对齐\n    \\hline % 表示画一条表格的横线\n  \\end{tabular}\n\\end{table}\n\n\n\n以块的方式显示信息。普通块与警告块。\n\\begin{block}{块名称}\n   显示内容\n\\end{block}\n\n\\begin{alertblock}{块名称} % 显示未红色\n   显示内容\n\\end{alertblock}\n\n\n\n使用listings软件包 + 设置代码风格 - 设置字体 - 设置代码字体颜色 - 设置代码风格 - 应用代码风格 + 实际代码环境\n\n设置字体，颜色以及代码风格并全局应用\n\n\\setmonofont{Monaco} % 设置等宽字体为Monaco\n\n% 设置颜色\n\\definecolor{codegreen}{rgb}{0, 0.6, 0}\n\\definecolor{codegray}{rgb}{0.5, 0.5, 0.5}\n\\definecolor{codepurple}{rgb}{0.58, 0, 0.82}\n\\definecolor{backcolor}{rgb}{0.95, 0.95, 0.92}\n\n% 代码风格设置\n\\lstdefinestyle{customstyle}{\n    backgroundcolor=\\color{backcolor},\n    commentstyle=\\color{codegreen},\n    keywordstyle=\\color{magenta},\n    numberstyle=\\color{codegray},\n    stringstyle=\\color{codepurple},\n    basicstyle=\\ttfamily\\footnotesize,\n    breakatwhitespace=false,\n    breaklines=true,\n    captionpos=b,\n    keepspaces=true,\n    numbers=left,\n    numbersep=3pt,\n    showspaces=flase,\n    showstringspaces=false,\n    showtabs=false,\n    tabsize=2,\n}\n% 应用代码风格\n\\lstset{style=customstyle}\n\n应用代码风格 在frame环境中\n\n\\lstinputlisting[\n  firstline=1, % 从文件导入代码，从文件的开始的行号\n  lastline=5, % 从文件导入代码，从文件的结束的行号\n  language=Python, % 编程语言\n  frame=single,\n  caption=First ten lines of some Python code, % 代码的说明\n  label=python\n  ]{perceptron_and.py} % 导入代码的文件\n\n\n\n\\hyperlink{链接文字描述}{链接地址}\n\n\n\n定理和推论环境会自动将其内容放在斜体字中。 beamer会自动加载amsmath软件包，所以可以直接添加数学公式。\n\\begin{theorem}[定理名称]\n$ a^2 + b^2 = c^2$\n\\end{theorem}\n\\begin{corollary} % 推论\n$ x + y = y + x  $\n\\end{corollary}\n\\begin{proof} % 论据\n$\\omega +\\phi = \\epsilon $\n\\end{proof}\n\n\n\n\n\n把Logo作为一个图片或者PDF文件\n以插入图片的方式进行插入\n对相关属性进行设置\n\n\n\n\n\n居中\n换行\n\n\n\n\\centering % 写在需要居中的环境命令上面一行\n\n\n\n\\\\\n\n\n\n\n\n使用xelatex将*.tex生成PDF文件\n使用ImageMagick的脚本将PDF转成PowerPoint\n\nxelatex demo.tex # 生成demo.pdf\n# https://github.com/ashafaei/pdf2pptx/blob/master/pdf2pptx.sh\n./pdf2pptx.sh demo.pdf # 生成 demo.pptx"
  },
  {
    "objectID": "posts/convert_latex_to_pptx/index.html#从markdownorgmode到pdf",
    "href": "posts/convert_latex_to_pptx/index.html#从markdownorgmode到pdf",
    "title": "从LaTex到PowerPoint",
    "section": "",
    "text": "Pandoc\nMarkdown/Orgmode -----------&gt;  PDF/epub\npandoc参数说明 - -f: 表示from, 表示源格式 - -t: 表示to, 表示目标格式 - -o: 表示输出文件名\npandoc -f org -t pdf -o demo.pdf demo.org\npandoc -f markdown -t pdf -o demo.pdf demo.md"
  },
  {
    "objectID": "posts/emacs_time_func/index.html",
    "href": "posts/emacs_time_func/index.html",
    "title": "Emacs获取当前系统时间生成文件名",
    "section": "",
    "text": "格式化当前时间函数\n\nFormat yyyy-mm-dd\n\n(format-time-string \"%Y-%m-%d\")\n;; \"2022-11-28\"\n\nUnix Time Format(number of seconds since 1970-01-01)\n\n(format-time-string \"%s\")\n;; \"1669620311\"\n\nNames for Month and Week\n\n(format-time-string \"%B\") ; \"November\"\n(format-time-string \"%b\") ; \"Nov\"\n(format-time-string \"%A\") ; \"Monday\"\n(format-time-string \"%a\") ; \"Tue\"\n\nTimezone\n\n(format-time-string \"%z\")\n\n\n拼接字符串(concat)\n(concat \"/path/to/root\" \"cur_dir\" \"file_name\" \".extend\")\n\n\n打开(创建)当天日期文件\n(open-file\n  (concat \"~/org\" (format-time-string \"%Y-%m-%d\") \".org\"))"
  },
  {
    "objectID": "posts/rust_toolchain/index.html",
    "href": "posts/rust_toolchain/index.html",
    "title": "Rust Toolchain常用功能介绍",
    "section": "",
    "text": "rustup常用命令\nrustup是Rust语言的一个安装器。\n可以管理Rust Toolchain以及rustup等工具链组件\n\n查看当前编译器信息\n\nrustup show\n\n更新Rust Toolchain\n\nrust update\n\n检查更新Rust Toolchain和rustup\n\nrustup check\n\n设置默认工具链\n\nrustup default -h\n\n管理component\n\n\n安装组件\n\nrust component add &lt;component_name&gt;\n\n删除组件\n\nrust component remove &lt;component_name&gt;\n\n列出所有可用组件\n\nrust component list\n\n列出以安装组件\n\nrust component list --installed\n\n\nClippy\n一系列lints，用于捕获常见错误并改进Rust代码。\n\n安装\nrustup update\nrustup component add clippy\n\n\n运行Clippy\ncargo clippy\n或者\ncargo check\n\n\n自动应用Clippy建议\ncargo clippy --fix\n\n\n配置文件\n\nclippy.toml\n.clippy.toml\n\n\n\n\nRustfmt\nRust代码格式化工具\n\n安装\nrustup component add rustfmt\n\n\n运行\ncargo fmt\n\n\n配置文件\n\nrustfmt.toml\n.rustfmt.toml\n\n\n\n\nRust-Analyzer\nrust-analyzer是Rust编程语言的语言服务器协议的实现。"
  },
  {
    "objectID": "posts/program_basic/index.html",
    "href": "posts/program_basic/index.html",
    "title": "Programming Basic Concept",
    "section": "",
    "text": "只读数据段(RODATA/RDATE)\n堆(Heap)\n栈(Stack)\n\n\n只读数据段\n\n存放常量\n\nStack\n\n是程序运行的基础\n每当一个函数被调用时，一块连续的内存就会在栈顶被分配出来，这块内存被称为帧(frame).\n自顶向下增长,出去入口帧(entry frame),就是main()对应的帧,随着main()函数一层层调用,栈会一层层扩展,调用结束,栈又会一层层回溯,把内存释放\n调用过程中,一个新的帧会分配足够的空间存储寄存器的上下文\n在函数里使用到的通用寄存器在栈保存一个副本，函数调用结束,通过副本,可以恢复出原本的寄存器的上下文\n函数的局部变量也都会在帧分配的时候被预留出来\n\nHeap\n\n当我们需要动态大小的内存时，只能用堆\n动态生命周期的内存也需要分配到堆上\n堆上分配出来的每一块内存需要显式地释放，这就使内存有更加灵活的生命周期\n\n\n\n\n\n在编译时，一切无法确定大小或者可以改变的数据，都无法安全地放在栈上，最好放在堆上\n只要可能，我们应该把变量分配到栈上，这样可以达到更好的运行速度\n栈上存放的数据是静态的，固定大小，固定生命周期，在编译期可以确定\n堆上存放的数据是动态的，不固定大小，不固定生命周期，在编译期不能确定"
  },
  {
    "objectID": "posts/program_basic/index.html#内存memory",
    "href": "posts/program_basic/index.html#内存memory",
    "title": "Programming Basic Concept",
    "section": "",
    "text": "只读数据段(RODATA/RDATE)\n堆(Heap)\n栈(Stack)\n\n\n只读数据段\n\n存放常量\n\nStack\n\n是程序运行的基础\n每当一个函数被调用时，一块连续的内存就会在栈顶被分配出来，这块内存被称为帧(frame).\n自顶向下增长,出去入口帧(entry frame),就是main()对应的帧,随着main()函数一层层调用,栈会一层层扩展,调用结束,栈又会一层层回溯,把内存释放\n调用过程中,一个新的帧会分配足够的空间存储寄存器的上下文\n在函数里使用到的通用寄存器在栈保存一个副本，函数调用结束,通过副本,可以恢复出原本的寄存器的上下文\n函数的局部变量也都会在帧分配的时候被预留出来\n\nHeap\n\n当我们需要动态大小的内存时，只能用堆\n动态生命周期的内存也需要分配到堆上\n堆上分配出来的每一块内存需要显式地释放，这就使内存有更加灵活的生命周期\n\n\n\n\n\n在编译时，一切无法确定大小或者可以改变的数据，都无法安全地放在栈上，最好放在堆上\n只要可能，我们应该把变量分配到栈上，这样可以达到更好的运行速度\n栈上存放的数据是静态的，固定大小，固定生命周期，在编译期可以确定\n堆上存放的数据是动态的，不固定大小，不固定生命周期，在编译期不能确定"
  },
  {
    "objectID": "posts/program_basic/index.html#数据",
    "href": "posts/program_basic/index.html#数据",
    "title": "Programming Basic Concept",
    "section": "数据",
    "text": "数据\n\n值\n类型\n指针\n引用\n\n\n值\n\n\n类型是对值的区分,包含了值在内存中的长度,对齐以及值可以进行操作等信息\n一个值是符合一个特定类型的数据的某个实体\n值是无法脱离具体的类型讨论的\n\n\n类型\n\n\n原生类型(primitive type): 固定大小,可以分配在栈上\n\n字符\n整数\n浮点数\n布尔值\n数组\n元组\n枚举\n指针: 指针的使用限制少，如果没有正确的类型解引用一个指针，会引发各种内存问题，造成潜在安全漏洞\n引用: 引用的解引用访问是受限的，它只能解引用到它引用数据的类型，不能用作它用\n\n组合类型(composite type)\n\n结构体: 多个类型组合在一起共同表达一个值的复杂数据结构\n标签联合: 可以存储一组不同但固定的类型的类型中的某个类型的对象，具体是由标签决定\n\n其他类型\n\n函数\n闭包"
  },
  {
    "objectID": "posts/program_basic/index.html#代码",
    "href": "posts/program_basic/index.html#代码",
    "title": "Programming Basic Concept",
    "section": "代码",
    "text": "代码\n\n函数: 对代码中重复行为的抽象\n方法: 在类中或者对象中定义的函数，只为某种类型定义的函数,和对象的指针发生关联\n闭包: 将函数或者说代码和其环境(自由变量)一起存储的一种数据结构。\n接口: 将使用方和实现方隔离开来，使两者不直接有依赖关系，大大提高了复用性和扩展性\n虚表: 存储多态接口方法的列表,辅助运行时代码执行，方便进行动态分派，是运行时多态的基础\n\n面向接口的设计师软件开发中的重要能力，当我们在运行期使用接口来引用具体类型的时候，代码就具备了运行时多态的能力。 在运行时，一旦使用了关于接口的引用，变量原本的类型被抹去，我们无法单纯从一个指针分析出这个引用具备什么样的能力。 在生成这个引用的时候，我们需要构建胖指针，除了指向数据本身，还需要指向一张涵盖这个接口所支持方法的列表。 这个列表就是我们熟知的虚表(virtual table)"
  },
  {
    "objectID": "posts/program_basic/index.html#运行方式",
    "href": "posts/program_basic/index.html#运行方式",
    "title": "Programming Basic Concept",
    "section": "运行方式",
    "text": "运行方式\n\n并发: 是一种能力, 同时与多件事情打交道的能力(交替执行多件不同的事情)\n并行: 是一种手段, 在多个CPU Core上执行同样的代码\n同步: 一个任务开始后，后续的操作会阻塞，直到这个任务结束，保证了代码的因果关系，保证程序正确性\n异步: 指一个任务开始执行后，与它没有因果关系的其它任务可以正常执行，不必等待前一个任务结束\nPromise / Future / Delay / Deferred: 是一个对象，用来描述在未来某个时刻才能获得的结果的值\nasync / await: async用来定义一个可以并发执行的任务, await则触发这个任务并执行\n\n在遭遇I/O处理时，高效CPU指令和低效I/O之间的巨大鸿沟，成为了软件的性能杀手。\n和内存访问相比，I/O操作的访问速度低了两个数量级,一旦遇到I/O操作，CPU就只能闲置来等待I/O设备运行完毕。\n因此操作系统为应用程序提供了异步I/O，让应用可以在当前I/O处理完毕之前，将CPU时间用作其它任务的处理\n很多拥有高并发处理能力的编程语言，会在用户程序中嵌入一个M:N的调度器，把M个并发任务，合理地分配在N个CPU Core上并行运行， 让程序的吞吐量达到最大。比如Golang的Goroutine\nPromise一般存在三个状态: - 初始状态, Promise还未运行 - 等待(pending)状态, Promise已运行，还未结束 - 结束状态, Promise成功解析出一个值，或者执行失败"
  },
  {
    "objectID": "posts/program_basic/index.html#编程范式",
    "href": "posts/program_basic/index.html#编程范式",
    "title": "Programming Basic Concept",
    "section": "编程范式",
    "text": "编程范式\n\n泛型编程\n\n数据结构的泛型: 把参数化数据结构理解成一个产生类型的函数，是一种高级抽象\n代码的泛型化: 当我们使用泛型结构编写代码时，相关的代码也需要额外的抽象\n\n通过参数化让数据结构像函数一样延迟绑定，提升通用性，类型的参数可以用接口约束，使类型满足一定的行为， 同时，在使用泛型结构时，我们的代码也需要更高的抽象度。"
  },
  {
    "objectID": "rust_sdl2/index.html",
    "href": "rust_sdl2/index.html",
    "title": "Rust SDL2案例",
    "section": "",
    "text": "案例来自于Rust Programming by Example,由于按照书籍的操作会存在一些错误，所以用此博客来记录更多的细节。\n\n\n\nMacOS M2: 13.5.2\nRust: 1.70.0\n\n\n\n\n\n创建工程\n\ncargo new tetris\n\n添加依赖于Cargo.toml\n\n[package]\nname = \"tetris\"\nversion = \"0.0.1\"\n\n[dependencies]\nsdl2 = \"0.34.5\"\n\n安装SDL2\n\nbrew install sdl2\n\n配置环境变量\n\nexport RUSTFLAGS=\"-L /opt/homebrew/lib\"\n此处RUSTFLAGS环境变量的内容需要根据自己的SDL2库的安装路径来配置，由于我的库安装路径是/opt/homebrew/lib下。 执行cargo build编译程序会依赖SDL的动态库；同时我也尝试了其他两种方法并没有成功\n其它方法一： 设置环境变量DYLD_LIBRARY_PATH\nexport DYLD_LIBRARY_PATH=/opt/homebrew/lib\n其它方法二： 在Cargo.toml中添加\n[build]\nrustc-link-search = [\"/opt/homebrew/lib\"]\n\n基础示例代码\n\nextern crate sdl2;\n\nuse sdl2::event::Event;\nuse sdl2::keyboard::Keycode;\nuse sdl2::pixels::Color;\nuse std::time::Duration;\n\nfn main() {\n    // 初始化SDL2\n    let sdl_context = sdl2::init().unwrap();\n    let video_subsystem = sdl_context.video().unwrap();\n\n    // 创建窗口和画布\n    let window = video_subsystem\n        .window(\"SDL2 Window\", 800, 600)\n        .position_centered()\n        .build()\n        .unwrap();\n\n    let mut canvas = window.into_canvas().build().expect(\"Failed to convert window into canvas\");\n\n    // 渲染代码\n    canvas.set_draw_color(Color::RGB(255, 0, 0));\n    canvas.clear();\n    canvas.present();\n\n    // 创建事件处理器\n    let mut event_pump = sdl_context.event_pump().expect(\"Failed to get SDL event pump\");\n\n    // 主循环\n    'runningloop: loop {\n        for event in event_pump.poll_iter() {\n            match event {\n                Event::Quit { .. } | Event::KeyDown { keycode: Some(Keycode::Escape), .. } =&gt; {\n                    break 'runningloop;\n                }\n                _ =&gt; {}\n            }\n        }\n\n        // 添加延迟以控制帧率\n        std::thread::sleep(Duration::new(0, 1_000_000_000u32 / 60));\n    }\n}\n\n构建程序\n\ncargo build\n\n运行程序\n\ncargo run\n运行结果如下图： \n\n实现过程\n\n\n导入外部crate SDL2\nextern crate sdl2;\n初始化SDL context\nlet sdl_context = sdl2::init().expect(\"SDL initialization failed\");\n获取video subsystem\nlet video_subsystem = sdl_context.video().expect(\"Couldn't get SDL\n       video subsystem\");\n创建windows\nlet window = video_subsystem.window(\"Tetris\", 800, 600)\n                             .position_centered()\n                             .opengl()\n                             .build()\n                             .expect(\"Failed to create window\");\n\nthe parameters for the window method\n\ntitle\nwidth\nheight\n\n.position_centered() method\n\n在屏幕中间获取窗口\n\n.opengl()\n\n让SDL使用opengl渲染\n\n.build()\n\n根据前面提供的参数创建窗口\n\n.expect()\n\n处理异常\n\n\n事件循环 正常情况下，展示一个窗口并关闭特别快，我们需要添加时间循环确保窗口一直在运行。\n\n导入必要相关的库\n\nuse sdl2::event::Event;\nuse sdl2::keyboard::Keycode;\n\nuse std::thread::sleep;\nuse std::time::Duration;\n\n获取时间循环管理器\n\nlet mut event_pump = sdl_context.event_pump().expect(\"Failed to\n        get SDL event pump\");\n\n创建无限循环循环事件\n\n'running: loop {\n     for event in event_pump.poll_iter() {\n         match event {\n             Event::Quit { .. } |\n             Event::KeyDown { keycode: Some(Keycode::Escape), .. } =&gt; {\n                 break 'running // We \"break\" the infinite loop.\n             },\n             _ =&gt; {} \n         }\n     }\n     sleep(Duration::new(0, 1_000_000_000u32 / 60));\n}\nrunning是一个循环跳出的标签(label) 收到一个quit event或者按Esc键，程序退出\n\n\n\n\n\n当我们有了一个窗口(window)时，我们需要获取(get)窗口的画布(window’s canvas)\n\nlet mut canvas = window.into_canvas()\n                 .target_texture()\n                 .present_vsync()\n                 .build()\n                 .expect(\"Couldn't get window's canvas\");\n以上代码的简单说明：\n\ninto_canvas: 将窗口(window)转换为画布(canvas)，以便我们可以轻松操作它\ntarget_texture: 激活纹理渲染支持\npresent_vsync: 允许v-sync(竖直同步操作)限制\nbuild: 应用前面设置的参数创建画布(canvas)\n\n\n\n\n当我们有了一个窗口的画布时，我们可以创建纹理，粘贴(paste onto)在其上。 获取(get)一个纹理创造器(texture creator)\n\n添加包含的结构\nuse sdl2::render::{Canvas, Texture, TextureCreator};\n获取纹理创造器\nlet texture_creator: TextureCreator&lt;_&gt; = canvas.texture_creator();\n\n\n\n\n首先创建一个常量确定矩形的尺寸，然后使用纹理创造器创建一个矩形的纹理\n\n设置矩形矩形的尺寸\nconst TEXTURE_SIZE: u32 = 32; // 矩形的尺寸大小 \n使用纹理创造器创建矩形\nconst TEXTURE_SIZE: u32 = 32; // 矩形的尺寸大小\nlet mut square_texture: Texture =\n   texture_creator.create_texture_target(None, TEXTURE_SIZE,\n     TEXTURE_SIZE)\n   .expect(\"Failed to create a texture\");\n\n\n\n\n要为纹理设置颜色，需要引入颜色模块的结构，然后使用画布设置纹理的颜色，颜色绘制完成后需要清空纹理；\n\n引入颜色结构\nuse sdl2::pixels::Color;\n使用画布设置矩形纹理的颜色\ncanvas.with_texture_canvas(&mut square_texture, |texture| {\n  texture.set_draw_color(Color::RGB(0, 255, 0));\n  texture.clear();\n})\n\n代码的简单说明:\n为了更新窗口的渲染内容，我们需要在将代码包含在main loop(and after the event loop). clear()纹理，以至于它被填充绿色\n\n将窗口填充红色\ncanvas.set_draw_color(Color::RGB(255, 0, 0));\ncanvas.clear();\n将纹理拷贝到窗口中\ncanvas.copy(&square_texture, None, Rect::new(0, 0, TEXTURE_SIZE, TEXTURE_SIZE))\n      .expect(\"Couldn't copy texture into window\");\n最后更新窗口的展示\ncanvas.present();\n\n代码如下：\nextern crate sdl2;\n\nuse sdl2::event::Event;\nuse sdl2::keyboard::Keycode;\nuse sdl2::pixels::Color;\nuse sdl2::render::{Texture, TextureCreator};\nuse sdl2::rect::Rect;\nuse std::time::Duration;\n\nfn main() {\n     // 初始化SDL2\n     let sdl_context = sdl2::init().expect(\"SDL initialization failed\");\n     let video_subsystem = sdl_context.video().expect(\"Couldn't get SDL video subsystem\");\n\n     // 创建窗口和画布\n     let window = video_subsystem\n         .window(\"Tetris\", 800, 600)\n         .position_centered()\n         .build()\n         .expect(\"Failed to create window\");\n\n     let mut canvas = window.into_canvas()\n         .target_texture()\n         .present_vsync()\n         .build()\n         .expect(\"Couldn't get window's canvas\");\n\n     let texture_creator: TextureCreator&lt;_&gt; = canvas.texture_creator();\n     const TEXTURE_SIZE: u32 = 32;\n\n     // create a texture with a 32*32 size\n     let mut square_texture: Texture = texture_creator.create_texture_target(None, TEXTURE_SIZE, TEXTURE_SIZE)\n         .expect(\"Failed to create a texture\");\n\n     // use the canvas to draw into our square texture.\n     canvas.with_texture_canvas(&mut square_texture, |texture| {\n         // set the draw color to green\n         texture.set_draw_color(Color::RGB(0, 255, 0));\n         texture.clear();\n     }).expect(\"Failed to clear a texture\");\n\n     // 创建事件处理器\n     let mut event_pump = sdl_context.event_pump().expect(\"Failed to get SDL event pump\");\n\n     // 主循环\n     'runningloop: loop {\n         for event in event_pump.poll_iter() {\n             match event {\n                 Event::Quit { .. } | Event::KeyDown { keycode: Some(Keycode::Escape), .. } =&gt; {\n                     break 'runningloop;\n                 }\n                 _ =&gt; {}\n             }\n         }\n\n         // We set fulfill our window with red\n         canvas.set_draw_color(Color::RGB(255, 0, 0));\n         // We draw it\n         canvas.clear();\n         // Copy our texture into the window\n         canvas.copy(&square_texture, None, Rect::new(0, 0, TEXTURE_SIZE, TEXTURE_SIZE))\n             .expect(\"Couldn't copy texture into window\");\n         canvas.present();\n\n         // 添加延迟以控制帧率\n         std::thread::sleep(Duration::new(0, 1_000_000_000u32 / 60));\n     }\n}\n\n代码执行结果如下图所示:"
  },
  {
    "objectID": "rust_sdl2/index.html#实验环境",
    "href": "rust_sdl2/index.html#实验环境",
    "title": "Rust SDL2案例",
    "section": "",
    "text": "MacOS M2: 13.5.2\nRust: 1.70.0"
  },
  {
    "objectID": "rust_sdl2/index.html#初始化开发环境",
    "href": "rust_sdl2/index.html#初始化开发环境",
    "title": "Rust SDL2案例",
    "section": "",
    "text": "创建工程\n\ncargo new tetris\n\n添加依赖于Cargo.toml\n\n[package]\nname = \"tetris\"\nversion = \"0.0.1\"\n\n[dependencies]\nsdl2 = \"0.34.5\"\n\n安装SDL2\n\nbrew install sdl2\n\n配置环境变量\n\nexport RUSTFLAGS=\"-L /opt/homebrew/lib\"\n此处RUSTFLAGS环境变量的内容需要根据自己的SDL2库的安装路径来配置，由于我的库安装路径是/opt/homebrew/lib下。 执行cargo build编译程序会依赖SDL的动态库；同时我也尝试了其他两种方法并没有成功\n其它方法一： 设置环境变量DYLD_LIBRARY_PATH\nexport DYLD_LIBRARY_PATH=/opt/homebrew/lib\n其它方法二： 在Cargo.toml中添加\n[build]\nrustc-link-search = [\"/opt/homebrew/lib\"]\n\n基础示例代码\n\nextern crate sdl2;\n\nuse sdl2::event::Event;\nuse sdl2::keyboard::Keycode;\nuse sdl2::pixels::Color;\nuse std::time::Duration;\n\nfn main() {\n    // 初始化SDL2\n    let sdl_context = sdl2::init().unwrap();\n    let video_subsystem = sdl_context.video().unwrap();\n\n    // 创建窗口和画布\n    let window = video_subsystem\n        .window(\"SDL2 Window\", 800, 600)\n        .position_centered()\n        .build()\n        .unwrap();\n\n    let mut canvas = window.into_canvas().build().expect(\"Failed to convert window into canvas\");\n\n    // 渲染代码\n    canvas.set_draw_color(Color::RGB(255, 0, 0));\n    canvas.clear();\n    canvas.present();\n\n    // 创建事件处理器\n    let mut event_pump = sdl_context.event_pump().expect(\"Failed to get SDL event pump\");\n\n    // 主循环\n    'runningloop: loop {\n        for event in event_pump.poll_iter() {\n            match event {\n                Event::Quit { .. } | Event::KeyDown { keycode: Some(Keycode::Escape), .. } =&gt; {\n                    break 'runningloop;\n                }\n                _ =&gt; {}\n            }\n        }\n\n        // 添加延迟以控制帧率\n        std::thread::sleep(Duration::new(0, 1_000_000_000u32 / 60));\n    }\n}\n\n构建程序\n\ncargo build\n\n运行程序\n\ncargo run\n运行结果如下图： \n\n实现过程\n\n\n导入外部crate SDL2\nextern crate sdl2;\n初始化SDL context\nlet sdl_context = sdl2::init().expect(\"SDL initialization failed\");\n获取video subsystem\nlet video_subsystem = sdl_context.video().expect(\"Couldn't get SDL\n       video subsystem\");\n创建windows\nlet window = video_subsystem.window(\"Tetris\", 800, 600)\n                             .position_centered()\n                             .opengl()\n                             .build()\n                             .expect(\"Failed to create window\");\n\nthe parameters for the window method\n\ntitle\nwidth\nheight\n\n.position_centered() method\n\n在屏幕中间获取窗口\n\n.opengl()\n\n让SDL使用opengl渲染\n\n.build()\n\n根据前面提供的参数创建窗口\n\n.expect()\n\n处理异常\n\n\n事件循环 正常情况下，展示一个窗口并关闭特别快，我们需要添加时间循环确保窗口一直在运行。\n\n导入必要相关的库\n\nuse sdl2::event::Event;\nuse sdl2::keyboard::Keycode;\n\nuse std::thread::sleep;\nuse std::time::Duration;\n\n获取时间循环管理器\n\nlet mut event_pump = sdl_context.event_pump().expect(\"Failed to\n        get SDL event pump\");\n\n创建无限循环循环事件\n\n'running: loop {\n     for event in event_pump.poll_iter() {\n         match event {\n             Event::Quit { .. } |\n             Event::KeyDown { keycode: Some(Keycode::Escape), .. } =&gt; {\n                 break 'running // We \"break\" the infinite loop.\n             },\n             _ =&gt; {} \n         }\n     }\n     sleep(Duration::new(0, 1_000_000_000u32 / 60));\n}\nrunning是一个循环跳出的标签(label) 收到一个quit event或者按Esc键，程序退出"
  },
  {
    "objectID": "rust_sdl2/index.html#画布canvas",
    "href": "rust_sdl2/index.html#画布canvas",
    "title": "Rust SDL2案例",
    "section": "",
    "text": "当我们有了一个窗口(window)时，我们需要获取(get)窗口的画布(window’s canvas)\n\nlet mut canvas = window.into_canvas()\n                 .target_texture()\n                 .present_vsync()\n                 .build()\n                 .expect(\"Couldn't get window's canvas\");\n以上代码的简单说明：\n\ninto_canvas: 将窗口(window)转换为画布(canvas)，以便我们可以轻松操作它\ntarget_texture: 激活纹理渲染支持\npresent_vsync: 允许v-sync(竖直同步操作)限制\nbuild: 应用前面设置的参数创建画布(canvas)"
  },
  {
    "objectID": "rust_sdl2/index.html#纹理texture",
    "href": "rust_sdl2/index.html#纹理texture",
    "title": "Rust SDL2案例",
    "section": "",
    "text": "当我们有了一个窗口的画布时，我们可以创建纹理，粘贴(paste onto)在其上。 获取(get)一个纹理创造器(texture creator)\n\n添加包含的结构\nuse sdl2::render::{Canvas, Texture, TextureCreator};\n获取纹理创造器\nlet texture_creator: TextureCreator&lt;_&gt; = canvas.texture_creator();"
  },
  {
    "objectID": "rust_sdl2/index.html#创建矩形",
    "href": "rust_sdl2/index.html#创建矩形",
    "title": "Rust SDL2案例",
    "section": "",
    "text": "首先创建一个常量确定矩形的尺寸，然后使用纹理创造器创建一个矩形的纹理\n\n设置矩形矩形的尺寸\nconst TEXTURE_SIZE: u32 = 32; // 矩形的尺寸大小 \n使用纹理创造器创建矩形\nconst TEXTURE_SIZE: u32 = 32; // 矩形的尺寸大小\nlet mut square_texture: Texture =\n   texture_creator.create_texture_target(None, TEXTURE_SIZE,\n     TEXTURE_SIZE)\n   .expect(\"Failed to create a texture\");"
  },
  {
    "objectID": "rust_sdl2/index.html#设置颜色",
    "href": "rust_sdl2/index.html#设置颜色",
    "title": "Rust SDL2案例",
    "section": "",
    "text": "要为纹理设置颜色，需要引入颜色模块的结构，然后使用画布设置纹理的颜色，颜色绘制完成后需要清空纹理；\n\n引入颜色结构\nuse sdl2::pixels::Color;\n使用画布设置矩形纹理的颜色\ncanvas.with_texture_canvas(&mut square_texture, |texture| {\n  texture.set_draw_color(Color::RGB(0, 255, 0));\n  texture.clear();\n})\n\n代码的简单说明:\n为了更新窗口的渲染内容，我们需要在将代码包含在main loop(and after the event loop). clear()纹理，以至于它被填充绿色\n\n将窗口填充红色\ncanvas.set_draw_color(Color::RGB(255, 0, 0));\ncanvas.clear();\n将纹理拷贝到窗口中\ncanvas.copy(&square_texture, None, Rect::new(0, 0, TEXTURE_SIZE, TEXTURE_SIZE))\n      .expect(\"Couldn't copy texture into window\");\n最后更新窗口的展示\ncanvas.present();\n\n代码如下：\nextern crate sdl2;\n\nuse sdl2::event::Event;\nuse sdl2::keyboard::Keycode;\nuse sdl2::pixels::Color;\nuse sdl2::render::{Texture, TextureCreator};\nuse sdl2::rect::Rect;\nuse std::time::Duration;\n\nfn main() {\n     // 初始化SDL2\n     let sdl_context = sdl2::init().expect(\"SDL initialization failed\");\n     let video_subsystem = sdl_context.video().expect(\"Couldn't get SDL video subsystem\");\n\n     // 创建窗口和画布\n     let window = video_subsystem\n         .window(\"Tetris\", 800, 600)\n         .position_centered()\n         .build()\n         .expect(\"Failed to create window\");\n\n     let mut canvas = window.into_canvas()\n         .target_texture()\n         .present_vsync()\n         .build()\n         .expect(\"Couldn't get window's canvas\");\n\n     let texture_creator: TextureCreator&lt;_&gt; = canvas.texture_creator();\n     const TEXTURE_SIZE: u32 = 32;\n\n     // create a texture with a 32*32 size\n     let mut square_texture: Texture = texture_creator.create_texture_target(None, TEXTURE_SIZE, TEXTURE_SIZE)\n         .expect(\"Failed to create a texture\");\n\n     // use the canvas to draw into our square texture.\n     canvas.with_texture_canvas(&mut square_texture, |texture| {\n         // set the draw color to green\n         texture.set_draw_color(Color::RGB(0, 255, 0));\n         texture.clear();\n     }).expect(\"Failed to clear a texture\");\n\n     // 创建事件处理器\n     let mut event_pump = sdl_context.event_pump().expect(\"Failed to get SDL event pump\");\n\n     // 主循环\n     'runningloop: loop {\n         for event in event_pump.poll_iter() {\n             match event {\n                 Event::Quit { .. } | Event::KeyDown { keycode: Some(Keycode::Escape), .. } =&gt; {\n                     break 'runningloop;\n                 }\n                 _ =&gt; {}\n             }\n         }\n\n         // We set fulfill our window with red\n         canvas.set_draw_color(Color::RGB(255, 0, 0));\n         // We draw it\n         canvas.clear();\n         // Copy our texture into the window\n         canvas.copy(&square_texture, None, Rect::new(0, 0, TEXTURE_SIZE, TEXTURE_SIZE))\n             .expect(\"Couldn't copy texture into window\");\n         canvas.present();\n\n         // 添加延迟以控制帧率\n         std::thread::sleep(Duration::new(0, 1_000_000_000u32 / 60));\n     }\n}\n\n代码执行结果如下图所示:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "曳影",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 24, 2023\n\n\n从LaTex到PowerPoint\n\n\n曳影 \n\n\n\n\nDec 30, 2022\n\n\nPoetry & Github Action\n\n\n曳影 \n\n\n\n\nDec 9, 2022\n\n\nRust Toolchain常用功能介绍\n\n\n曳影 \n\n\n\n\nDec 2, 2022\n\n\nProgramming Basic Concept\n\n\n曳影 \n\n\n\n\nNov 28, 2022\n\n\nEmacs获取当前系统时间生成文件名\n\n\n曳影 \n\n\n\n\nNov 23, 2022\n\n\nDoom Emacs & Poetry & Pyright管理Python开发环境\n\n\n曳影 \n\n\n\n\nNov 19, 2022\n\n\nKubernetes the hard way\n\n\n曳影 \n\n\n\n\nNov 15, 2022\n\n\n你好, Quarto\n\n\n曳影 \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/poetry_githubactions/index.html",
    "href": "posts/poetry_githubactions/index.html",
    "title": "Poetry & Github Action",
    "section": "",
    "text": "使用Poetry做Python的项目管理工具，集成到Github Actions，包含以下内容: - Check out 代码仓库 - 启动Python环境，可以限制版本，也可以使用多个版本 - 安装Poetry - 设置虚拟环境缓存 - 安装依赖 - 代码格式化(yapf) - 代码静态类型检查(pytype) - 代码测试，以及覆盖率测试报告\nGithub Action workflow 配置如下:\nname: CI\non: push\n\njobs:\n  ci:\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.8.5\"]\n        poetry-version: [\"1.2.2\"]\n        os: [ubuntu-18.04]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install Poetry\n        uses: snok/install-poetry@v1\n        with:\n          virtualenvs-create: true\n          virtualenvs-in-project: true\n          installer-parallel: true\n      - name: Load cached venv\n        id: cached-poetry-dependencies\n        uses: actions/cache@v3\n        with:\n          path: .venv\n          key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}\n\n      - name: Install Denpendencies\n        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'\n        run: poetry install --no-interaction --no-root\n\n      - name: Code Format\n        run: poetry run yapf\n\n      - name: Type Check\n        run: poetry run pytype --config=pytype.cfg\n\n      - name: Testing and coverage\n        run: poetry run pytest --cov"
  },
  {
    "objectID": "posts/kubernetes_the_hard_way/index.html",
    "href": "posts/kubernetes_the_hard_way/index.html",
    "title": "Kubernetes the hard way",
    "section": "",
    "text": "服务器说明\n\nKubernets Version\n\n\nv1.22.15\n\n\n节点要求\n\n\n节点数 &gt;= 3台\nCPUs &gt;= 2\nMemory &gt;= 2G\n\n\n修改时区 有的系统时区不匹配，需要修改\n\ntimedatectl set-timezone Asia/Shanghai\n\n环境说明\n\n\n\n\n系统类型\nIP地址\n节点角色\nCPU\nMemory\nHostname\n\n\n\n\nCentOS-7.9\n192.168.200.11\nmaster\n&gt;=2\n&gt;=2G\ncluster1\n\n\nCentOS-7.9\n192.168.200.22\nmaster,worker\n&gt;=2\n&gt;=2G\ncuster2\n\n\nCentOS-7.9\n192.168.200.33\nworker\n&gt;=2\n&gt;=2G\ncluster3\n\n\n\n\n使用Vagrant搭建虚拟机节点\n\n\nVagrant: latest\nVirtualBox: 7.0\nvagrant-vbguest: 0.21 (挂载host和guest同步目录)\n\nvagrant plugin install vagrant-vbguest --plugin-version 0.21\nVagrantfile配置文件如下:\n# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nnodes = [\n  {\n    :name =&gt; \"cluster1\",\n    :eth1 =&gt; \"192.168.200.11\",\n    :mem =&gt; \"4096\",\n    :cpu =&gt; \"2\"\n  },\n  {\n    :name =&gt; \"cluster2\",\n    :eth1 =&gt; \"192.168.200.22\",\n    :mem =&gt; \"4096\",\n    :cpu =&gt; \"2\"\n  },\n  {\n    :name =&gt; \"cluster3\",\n    :eth1 =&gt; \"192.168.200.33\",\n    :mem =&gt; \"4096\",\n    :cpu =&gt; \"2\"\n  },\n]\n\n\nVagrant.configure(\"2\") do |config|\n  # Every Vagrant development environment requires a box.\n  config.vm.box = \"centos/7\"\n\n  nodes.each do |opts|\n    config.vm.define opts[:name] do |config|\n      config.vm.hostname = opts[:name]\n\n      config.vm.provider \"virtualbox\" do |v|\n        v.customize [\"modifyvm\", :id, \"--memory\", opts[:mem]]\n        v.customize [\"modifyvm\", :id, \"--cpus\", opts[:cpu]]\n      end\n\n      #config.ssh.username = \"root\"\n      #config.ssh.private_key_path = \"/Users/jinpeng.d/.ssh/id_rsa\"\n      config.vm.synced_folder \"../share\", \"/vagrant_data\"\n\n      config.vm.network :public_network, ip: opts[:eth1]\n      config.vm.synced_folder \"../share\", \"/vagrant_data\"\n\n    end\n  end\nend\n\n\n系统设置(所有节点)\n\n所有操作需要root权限\nhostname (/etc/hosts)\n安装依赖包\n\nyum update -y\nyum install -y socat conntrack ipvsadm ipset jq sysstat curl iptables libseccomp yum-utils\n\n关闭防火墙,selinux, swap,重置 iptables\n\n# 1. 关闭selinux\nsetenforce 0\nsed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/config\n# 2. 关闭防火墙\nsystemctl stop firewalld && systemctl disable firewalld\n\n# 3. 设置ipttables规则\niptables -F && iptables -X && iptables -F -t nat && iptables -X -t nat && iptables -P FORWARD ACCEPT\n\n# 4. 关闭swap\nvi /etc/fstab\n# 永久禁用注释掉swap\n#/swapfile none swap defaults 0 0\n# 临时禁用\nswapoff -a\n# 这里两者都用，临时修改可以即时生效，不用重启，永久禁用防止重启后不生效\n\n# 5. 关闭dnsmasq(否则无法解析域名)\nservice dnsmasq stop && systemctl disable dnsmasq\n\nkubernetes参数设置\n\ncat &gt; /etc/sysctl.d/kubernetes.conf &lt;&lt;EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_nonlocal_bind = 1\nnet.ipv4.ip_forward = 1\nvm.swappiness = 0\nvm.overcommit_memory = 1\nEOF\n\n# 生效文件\nsysctl -p /etc/sysctl.d/kubernetes.conf\n\n配置免密登录 选择其中一个节点，或者一个单独的机器生成ssh公秘钥对，把公钥放在k8s所有节点服务器上\n\n# 生成公秘钥对, 如果没有可用的\nssh-keygen -t rsa\n\n# 查看公钥内容\ncat ~/.ssh/id_rsa.pub\n\n# 每一台节点机器上配置\necho \"&lt;pubkey content&gt;\" &gt;&gt; ~/.ssh/authorized_keys\n\n配置IP映射(每个节点)\n\ncat &gt; /etc/hosts &lt;&lt;EOF\n192.168.200.11 cluster1\n192.168.200.22 cluster2\n192.168.200.33 cluster3\nEOF\n\n下载k8s组件包\n\nexport VERSION=v1.22.15\n\n# 下载master节点组件\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-apiserver\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-controller-manager\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-scheduler\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kubectl\n\n# 下载worker节点组件\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-proxy\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kubelet\n\n# 下载etcd组件\nwget https://github.com/etcd-io/etcd/releases/download/v3.4.10/etcd-v3.4.10-linux-amd64.tar.gz\ntar -xvf etcd-v3.4.10-linux-amd64.tar.gz\nmv etcd-v3.4.10-linux-amd64/etcd* .\nrm -fr etcd-v3.4.10-linux-amd64*\n\n分发软件包\n\n# 把master相关组件分发到master节点\nMASTERS=(cluster1 cluster2)\nfor instance in ${MASTERS[@]}; do\n  scp kube-apiserver kube-controller-manager kube-scheduler kubectl root@${instance}:/usr/local/bin/\ndone\n\n# 把worker先关组件分发到worker节点\nWORKERS=(cluster2 cluster3)\nfor instance in ${WORKERS[@]}; do\n  scp kubelet kube-proxy root@${instance}:/usr/local/bin/\ndone\n\n# 把etcd组件分发到etcd节点\nETCDS=(cluster1 cluster2 cluster3)\nfor instance in ${ETCDS[@]}; do\n  scp etcd etcdctl root@${instance}:/usr/local/bin/\ndone\n\n\n生成证书\n\n准备工作\n\n\n安装cfssl\nwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfssl\nwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson\nchmod +x /usr/local/bin/cfssl*\nkubernetes集群组件\n\n服务端组件:\n\nkube-apiserver\nkube-controller-manager\nkube-scheduler\nkubectl\ncontainer runtime(containerd)\n\n客户端组件:\n\nkubelet\nkube-proxy\ncontainer runtime(containerd)\n\n\n\n\n生成根证书 根证书是集群所有节点共享的，只要创建一个CA证书，后续创建的所有证书都由它签名。 在可以登录到所有节点的控制台机器上创建pki目录存放证书\n\n\n根证书配置文件创建\ncat &gt; ca-config.json &lt;&lt;EOF\n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"876000h\"\n    },\n    \"profiles\": {\n      \"kubernetes\": {\n        \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"],\n        \"expiry\": \"876000h\"\n      }\n    }\n  }\n}\nEOF\n\ncat &gt; ca-csr.json &lt;&lt;EOF\n{\n  \"CN\": \"Kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"Portland\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"CA\",\n      \"ST\": \"Oregon\"\n    }\n  ]\n}\nEOF\n生成证书和密钥\n\n生成证书和私钥, ca.pem是证书, ca-key.pem是证书私钥 bash   cfssl gencert -initca ca-csr.json | cfssljson -bare ca 输出文件:\n\nca.pem\nca.csr\nca-key.pem\n\n\nadmin客户端证书\n\n\nadmin客户端证书配置文件\ncat &gt; admin-csr.json &lt;&lt;EOF\n{\n  \"CN\": \"admin\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"system:masters\",\n      \"OU\": \"seven\"\n    }\n  ]\n}\nEOF\n生成admin客户端证书和私钥(基于根证书和私钥以及证书配置文件)\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  admin-csr.json | cfssljson -bare admin\n输出文件:\n\nadmin.csr\nadmin.pem\nadmin-key.pem\n\n\n\nkubelet客户端证书 kubernetes使用一种称为Node Authorizer的专用授权模式来授权kubernetes发出的API请求。 kubelet使用将其标识为system:nodes组中的凭据,其用户名为system: node:nodeName, 接下来给每个Worker节点生成证书。\n\n\n生成kubelet客户端证书配置文件\n\n设置Worker节点列表\n\nWORKERS=(cluster2 cluster3)\nWORKER_IPS=(192.168.200.22 192.168.200.33)\nfor ((i=0;i&lt;${#WORKERS[@]};i++)); do\n  cat &gt; ${WORKERS[$i]}-csr.json &lt;&lt;EOF\n  {\n    \"CN\": \"system:node:${WORKERS[$i]}\",\n    \"key\": {\n      \"algo\": \"rsa\",\n      \"size\": 2048\n    },\n    \"names\": [\n      {\n        \"C\": \"CN\",\n        \"L\": \"Beijing\",\n        \"O\": \"system:nodes\",\n        \"OU\": \"seven\",\n        \"ST\": \"Beijing\"\n      }\n    ]\n  }\nEOF\ndone\n生成kubelet客户端证书和密钥\nfor ((i=0;i&lt;${#WORKERS[@]};i++)); do\n  cfssl gencert \\\n    -ca=ca.pem \\\n    -ca-key=ca-key.pem \\\n    -config=ca-config.json \\\n    -hostname=${WORKERS[$i]},${WORKER_IPS[$i]} \\\n    -profile=kubernetes \\\n    ${WORKERS[$i]}-csr.json | cfssljson -bare ${WORKERS[$i]}\ndone\n输出文件:\n\n{worker-node-name}.csr\n{worker-node-name}.pem\n{worker-node-name}-key.pem\n\n\n\nkube-controller-manager客户端证书\n\n\nkube-controller-manager客户端证书配置文件\n\ncat &gt; kube-controller-manager-csr.json &lt;&lt;EOF\n{\n    \"CN\": \"system:kube-controller-manager\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n      {\n        \"C\": \"CN\",\n        \"ST\": \"BeiJing\",\n        \"L\": \"BeiJing\",\n        \"O\": \"system:kube-controller-manager\",\n        \"OU\": \"seven\"\n      }\n    ]\n}\nEOF\n\n生成kube-controller-manager客户端证书\n\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-profile=kubernetes \\\nkube-controller-manager-csr.json | cfssljson -bare kube-controller-manager\n输出文件: + kube-controller-manager.csr + kube-controller-manager.pem + kube-controller-manager-key.pem\n\nkube-proxy客户端证书\n\n\nkube-proxy客户端证书配置文件\ncat &gt; kube-proxy-csr.json &lt;&lt;EOF\n{\n  \"CN\": \"system:kube-proxy\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"seven\"\n    }\n  ]\n}\nEOF\n生成kube-proxy客户端证书\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-profile=kubernetes \\\nkube-proxy-csr.json | cfssljson -bare kube-proxy\n输出文件:\n\nkube-proxy.csr\nkube-proxy.pem\nkube-proxy-key.pem\n\n\n\nkube-scheduler客户端证书\n\n\nkube-scheduler客户端证书配置\ncat &gt; kube-scheduler-csr.json &lt;&lt;EOF\n{\n    \"CN\": \"system:kube-scheduler\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n      {\n        \"C\": \"CN\",\n        \"ST\": \"BeiJing\",\n        \"L\": \"BeiJing\",\n        \"O\": \"system:kube-scheduler\",\n        \"OU\": \"seven\"\n      }\n    ]\n}\nEOF\n生成kube-scheduler客户端证书\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-profile=kubernetes \\\nkube-scheduler-csr.json | cfssljson -bare kube-scheduler\n输出文件:\n\nkube-scheduler.csr\nkube-scheduler.pem\nkube-scheduler-key.pem\n\n\n\nkube-apiserver服务端证书\n\n\nkube-apiserver服务端证书配置\ncat &gt; kubernetes-csr.json &lt;&lt;EOF\n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"seven\"\n    }\n  ]\n}\nEOF\n生成kube-apiserver服务端证书 服务端证书与客户端证书不同:\n\n客户端证书需要通过一个名字或者IP去访问服务端，所以证书需要包含客户端所访问的名字或IP，用以客户端验证\n指定可能作为master的节点服务地址\napiserver的 service ip地址(一般是svc网段的第一个ip)\n所有master内网IP和公网IP，逗号分割，(可以把所有节点写上，防止变换master节点)\n\nKUBERNETES_SVC_IP=\"10.233.0.1\"\nMASTER_IPS=\"192.168.200.11,192.168.200.22,192.168.200.33\"\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-hostname=${KUBERNETES_SVC_IP},${MASTER_IPS},127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local \\\n-profile=kubernetes \\\nkubernetes-csr.json | cfssljson -bare kubernetes\n输出文件:\n\nkubernetes.csr\nkubernetes.pem\nkubernetes-key.pem\n\n\n\nService Account证书\n\n\n配置文件\ncat &gt; service-account-csr.json &lt;&lt;EOF\n{\n  \"CN\": \"service-accounts\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"seven\"\n    }\n  ]\n}\nEOF\n生成证书\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-profile=kubernetes \\\nservice-account-csr.json | cfssljson -bare service-account\n输出文件:\n\nservice-account.csr\nservice-account.pem\nservice-account-key.pem\n\n\n\nproxy-client证书\n\n\n配置文件\ncat &gt; proxy-client-csr.json &lt;&lt;EOF\n{\n  \"CN\": \"aggregator\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"seven\"\n    }\n  ]\n}\nEOF\n生成证书\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-profile=kubernetes \\\nproxy-client-csr.json | cfssljson -bare proxy-client\n输出文件:\n\nproxy-client.csr\nproxy-client.pem\nproxy-client-key.pem\n\n\n\n分发客户端，服务端证书\n\n\n分发Worker节点需要的证书和私钥\n\n每个Worker节点证书和密钥\n\nWORKERS=(\"cluster2\" \"cluster3\")\nfor instance in ${WORKERS[@]}; do\n  scp ca.pem ${instance}-key.pem ${instance}.pem root@${instance}:~/\ndone\n分发Master节点需要的证书和私钥\n\n根证书和密钥(ca*.pem)\nkube-apiserver证书和密钥(kubenetes*.pem)\nservice-account证书和密钥(service-account*.pem)\nproxy-client证书和密钥(proxy-client*.pem)\n\nMASTER_IPS=(cluster1 cluster2)\nfor instance in ${MASTER_IPS[@]}; do\n  scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\\n    service-account-key.pem service-account.pem proxy-client.pem proxy-client-key.pem root@${instance}:~/\ndone\n\n\n\nkubernetes各组件的认证配置\nkubernetes的认证配置文件，称为kubeconfigs，用于让kubernetes的客户端定位 kube-apiserver并通过apiserver的安全认证。\n\ncontroller-manager\nkubelet\nkube-proxy\nscheduler\nadmin user\n\n\n为kubelet生成kubeconfigs(每个Worker节点)\n\nWORKERS=(\"cluster2\" \"cluster3\")\nfor instance in ${WORKERS[@]}; do\n  kubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://127.0.0.1:6443 \\\n    --kubeconfig=${instance}.kubeconfig\n\n  kubectl config set-credentials system:node:${instance} \\\n    --client-certificate=${instance}.pem \\\n    --client-key=${instance}-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=${instance}.kubeconfig\n\n  kubectl config set-context default \\\n    --cluster=kubernetes \\\n    --user=system:node:${instance} \\\n    --kubeconfig=${instance}.kubeconfig\n\n  kubectl config use-context default --kubeconfig=${instance}.kubeconfig\ndone\n\n为kube-proxy生成kubeconfigs\n\nkubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://127.0.0.1:6443 \\\n    --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config set-credentials system:kube-proxy \\\n  --client-certificate=kube-proxy.pem \\\n  --client-key=kube-proxy-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-proxy \\\n  --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig\n\n为kube-controller-manager生成kubeconfigs\n\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=ca.pem \\\n  --embed-certs=true \\\n  --server=https://127.0.0.1:6443 \\\n  --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-credentials system:kube-controller-manager \\\n  --client-certificate=kube-controller-manager.pem \\\n  --client-key=kube-controller-manager-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-controller-manager \\\n  --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig\n\n为kube-scheduler生成kubeconfigs\n\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=ca.pem \\\n  --embed-certs=true \\\n  --server=https://127.0.0.1:6443 \\\n  --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config set-credentials system:kube-scheduler \\\n  --client-certificate=kube-scheduler.pem \\\n  --client-key=kube-scheduler-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-scheduler \\\n  --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig\n\n为admin用户生成kubeconfigs\n\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=ca.pem \\\n  --embed-certs=true \\\n  --server=https://127.0.0.1:6443 \\\n  --kubeconfig=admin.kubeconfig\n\nkubectl config set-credentials admin \\\n  --client-certificate=admin.pem \\\n  --client-key=admin-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=admin.kubeconfig\n\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=admin \\\n  --kubeconfig=admin.kubeconfig\n\nkubectl config use-context default --kubeconfig=admin.kubeconfig\n\n分发kubeconfigs配置文件\n\n\nkubelet和kube-proxy的kubeconfigs分发到Worker节点\n\nWORKERS=(\"cluster2\" \"cluster3\")\nfor instance in ${WORKERS[@]}; do\n  scp ${instance}.kubeconfig kube-proxy.kubeconfig ${instance}:~/\ndone\n\nkube-controller-manager和kube-scheduler的kubeconfigs分发到Master节点\n\nMASTERS=(\"cluster1\" \"cluster2\")\nfor instance in ${MASTERS[@]}; do\n  scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${instance}:~/\ndone\n\n\n部署ETCD集群\n\n配置etcd证书文件\n\nmkdir -p /etc/etcd /var/lib/etcd\nchmod 700 /var/lib/etcd\ncp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/\n\n配置etcd.service文件\n\n\nECCD_IP: 为当前ectd存储服务器的IP\n\nETCD_NAME=$(hostname -s)\nETCD_IP=192.168.200.11\nETCD_NAMES=(cluster1 cluster2 cluster3)\nETCD_IPS=(192.168.200.11 192.168.200.22 192.168.200.33)\ncat &lt;&lt;EOF &gt; /etc/systemd/system/etcd.service\n[Unit]\nDescription=etcd\nDocumentation=https://github.com/coreos\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/etcd \\\\\n  --name ${ETCD_NAME} \\\\\n  --cert-file=/etc/etcd/kubernetes.pem \\\\\n  --key-file=/etc/etcd/kubernetes-key.pem \\\\\n  --peer-cert-file=/etc/etcd/kubernetes.pem \\\\\n  --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\\n  --trusted-ca-file=/etc/etcd/ca.pem \\\\\n  --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\\n  --peer-client-cert-auth \\\\\n  --client-cert-auth \\\\\n  --initial-advertise-peer-urls https://${ETCD_IP}:2380 \\\\\n  --listen-peer-urls https://${ETCD_IP}:2380 \\\\\n  --listen-client-urls https://${ETCD_IP}:2379,https://127.0.0.1:2379 \\\\\n  --advertise-client-urls https://${ETCD_IP}:2379 \\\\\n  --initial-cluster-token etcd-cluster-0 \\\\\n  --initial-cluster ${ETCD_NAMES[0]}=https://${ETCD_IPS[0]}:2380,${ETCD_NAMES[1]}=https://${ETCD_IPS[1]}:2380,${ETCD_NAMES[2]}=https://${ETCD_IPS[2]}:2380 \\\\\n  --initial-cluster-state new \\\\\n  --data-dir=/var/lib/etcd\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n启动etcd集群\n\nsystemctl daemon-reload && systemctl enable etcd && systemctl start etcd\n\n验证etcd集群\n\nETCDCTL_API=3 etcdctl member list \\\n--endpoints=https://127.0.0.1:2379 \\\n--cacert=/etc/etcd/ca.pem \\\n--cert=/etc/etcd/kubernetes.pem \\\n--key=/etc/etcd/kubernetes-key.pem\n成功输出结果如下(不同机器不同):\n87db4208f2c1d75, started, cluster2, https://192.168.200.22:2380, https://192.168.200.22:2379, false\n95c5677668e390bf, started, cluster1, https://192.168.200.11:2380, https://192.168.200.11:2379, false\nd728cb204dcd87a3, started, cluster3, https://192.168.200.33:2380, https://192.168.200.33:2379, false\n\n\n部署kubernetes控制面板\n每个组件有多个点保证高可用。 我们在cluster1和cluster2上部署kube-apiserver,kube-contronller-manager,kube-scheduler\n\n配置API Server(每个master节点都需要配置)\n\nmkdir -p /etc/kubernetes/ssl\n\nmv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\\n  service-account-key.pem service-account.pem \\\n  proxy-client.pem proxy-client-key.pem \\\n  /etc/kubernetes/ssl\n\nIP=192.168.200.11\nAPISERVER_COUNT=2\nETCD_ENDPOINTS=(192.168.200.11 192.168.200.22 192.168.200.33)\n\ncat &lt;&lt;EOF &gt; /etc/systemd/system/kube-apiserver.service\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n  --advertise-address=${IP} \\\\\n  --allow-privileged=true \\\\\n  --apiserver-count=${APISERVER_COUNT} \\\\\n  --audit-log-maxage=30 \\\\\n  --audit-log-maxbackup=3 \\\\\n  --audit-log-maxsize=100 \\\\\n  --audit-log-path=/var/log/audit.log \\\\\n  --authorization-mode=Node,RBAC \\\\\n  --bind-address=0.0.0.0 \\\\\n  --client-ca-file=/etc/kubernetes/ssl/ca.pem \\\\\n  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\\n  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\\\\n  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\\\\n  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\\\\n  --etcd-servers=https://${ETCD_ENDPOINTS[0]}:2379,https://${ETCD_ENDPOINTS[1]}:2379,https://${ETCD_ENDPOINTS[2]}:2379 \\\\\n  --event-ttl=1h \\\\\n  --kubelet-certificate-authority=/etc/kubernetes/ssl/ca.pem \\\\\n  --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \\\\\n  --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \\\\\n  --service-account-issuer=api \\\\\n  --service-account-key-file=/etc/kubernetes/ssl/service-account.pem \\\\\n  --service-account-signing-key-file=/etc/kubernetes/ssl/service-account-key.pem \\\\\n  --api-audiences=api,vault,factors \\\\\n  --service-cluster-ip-range=10.233.0.0/16 \\\\\n  --service-node-port-range=30000-32767 \\\\\n  --proxy-client-cert-file=/etc/kubernetes/ssl/proxy-client.pem \\\\\n  --proxy-client-key-file=/etc/kubernetes/ssl/proxy-client-key.pem \\\\\n  --runtime-config=api/all=true \\\\\n  --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \\\\\n  --requestheader-allowed-names=aggregator \\\\\n  --requestheader-extra-headers-prefix=X-Remote-Extra- \\\\\n  --requestheader-group-headers=X-Remote-Group \\\\\n  --requestheader-username-headers=X-Remote-User \\\\\n  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\\\\n  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\\\\n  --v=1\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n配置kube-controller-manager\n\nmv kube-controller-manager.kubeconfig /etc/kubernetes/\n\ncat &lt;&lt;EOF &gt; /etc/systemd/system/kube-controller-manager.service\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kube-controller-manager \\\\\n  --bind-address=0.0.0.0 \\\\\n  --cluster-cidr=10.200.0.0/16 \\\\\n  --cluster-name=kubernetes \\\\\n  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\\\\n  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\\\\n  --cluster-signing-duration=876000h0m0s \\\\\n  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\\n  --leader-elect=true \\\\\n  --root-ca-file=/etc/kubernetes/ssl/ca.pem \\\\\n  --service-account-private-key-file=/etc/kubernetes/ssl/service-account-key.pem \\\\\n  --service-cluster-ip-range=10.233.0.0/16 \\\\\n  --use-service-account-credentials=true \\\\\n  --v=1\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n配置kube-scheduler\n\nmv kube-scheduler.kubeconfig /etc/kubernetes\n\ncat &lt;&lt;EOF &gt; /etc/systemd/system/kube-scheduler.service\n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kube-scheduler \\\\\n  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\\n  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\\n  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\\n  --leader-elect=true \\\\\n  --bind-address=0.0.0.0 \\\\\n  --port=0 \\\\\n  --v=1\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n启动服务\n\nsystemctl daemon-reload\nsystemctl enable kube-apiserver\nsystemctl enable kube-controller-manager\nsystemctl enable kube-scheduler\nsystemctl start kube-apiserver\nsystemctl start kube-controller-manager\nsystemctl start kube-scheduler\n\n服务验证\n\nnetstat -ntlp\n正常输出如下:\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      18829/sshd\ntcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      853/master\ntcp        0      0 192.168.200.11:2379     0.0.0.0:*               LISTEN      30516/etcd\ntcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      30516/etcd\ntcp        0      0 192.168.200.11:2380     0.0.0.0:*               LISTEN      30516/etcd\ntcp6       0      0 ::1:25                  :::*                    LISTEN      853/master\ntcp6       0      0 :::6443                 :::*                    LISTEN      30651/kube-apiserve\ntcp6       0      0 :::10257                :::*                    LISTEN      30666/kube-controll\ntcp6       0      0 :::10259                :::*                    LISTEN      30679/kube-schedule\n\n配置kubectl kubectl用来管理kubernetes集群的客户端工具。\n\nmkdir ~/.kube/\nmv ~/admin.kubeconfig ~/.kube/config\nkubectl get nodes\n授权apiserver调用kubelet API,在执行kubectl exec/run/logs时apiserver会转发到kubelet\nkubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes\n\n\nDeploy kubernetes Worker Node\n每个节点上都会部署:\n\nkubelet\nkube-proxy\ncontainer runtime\ncni\nnginx-proxy\n\n\nContainer Runtine(containerd)\n\nVERSION=1.4.3\nwget https://github.com/containerd/containerd/releases/download/v${VERSION}/cri-containerd-cni-${VERSION}-linux-amd64.tar.gz\ntar -xvf cri-containerd-cni-${VERSION}-linux-amd64.tar.gz\ncp etc/crictl.yaml /etc/\ncp etc/systemd/system/containerd.service /etc/systemd/system/\ncp -r usr /\n\ncontainerd配置文件\n\nmkdir -p /etc/containerd\ncontainerd config default &gt; /etc/containerd/config.toml\n# Options\nvi /etc/containerd/config.toml\n\n启动containerd\n\nsystemctl enable containerd\nsystemctl start containerd\nsystemctl status containerd\n\n配置kubelet\n\n\n准备配置文件\n\nmkdir -p /etc/kubernetes/ssl/\nmv ${HOSTNAME}-key.pem ${HOSTNAME}.pem ca.pem ca-key.pem /etc/kubernetes/ssl/\nmv ${HOSTNAME}.kubeconfig /etc/kubernetes/kubeconfig\nIP=192.168.200.22\n\ncat &lt;&lt;EOF &gt; /etc/kubernetes/kubelet-config.yaml\nkind: KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    enabled: true\n  x509:\n    clientCAFile: \"/etc/kubernetes/ssl/ca.pem\"\nauthorization:\n  mode: Webhook\nclusterDomain: \"cluster.local\"\nclusterDNS:\n  - \"169.254.25.10\"\npodCIDR: \"10.200.0.0/16\"\naddress: ${IP}\nreadOnlyPort: 0\nstaticPodPath: /etc/kubernetes/manifests\nhealthzPort: 10248\nhealthzBindAddress: 127.0.0.1\nkubeletCgroups: /systemd/system.slice\nresolvConf: \"/etc/resolv.conf\"\nruntimeRequestTimeout: \"15m\"\nkubeReserved:\n  cpu: 200m\n  memory: 512M\ntlsCertFile: \"/etc/kubernetes/ssl/${HOSTNAME}.pem\"\ntlsPrivateKeyFile: \"/etc/kubernetes/ssl/${HOSTNAME}-key.pem\"\nEOF\n\n配置服务\n\ncat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=containerd.service\nRequires=containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n  --config=/etc/kubernetes/kubelet-config.yaml \\\\\n  --container-runtime=remote \\\\\n  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\\n  --image-pull-progress-deadline=2m \\\\\n  --kubeconfig=/etc/kubernetes/kubeconfig \\\\\n  --network-plugin=cni \\\\\n  --node-ip=${IP} \\\\\n  --register-node=true \\\\\n  --v=2\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n配置nginx-proxy nginx-proxy是有一个用于Worker节点访问apiserver的一个代理， 是apiserver一个优雅的高可用方案。 它使用kubelet的staticpod方式启动，让每个节点都可以均衡的访问每个apiserver服务 优雅的替代了通过虚拟ip访问apiserver的方式。\n\n只需要在没有``apiserver的节点部署\n注意stream中有几个节点写几个\nmkdir -p /etc/nginx\nMASTER_IPS=(10.155.19.223 10.155.19.64)\n\ncat &lt;&lt;EOF &gt; /etc/nginx/nginx.conf\nerror_log stderr notice;\n\nworker_processes 2;\nworker_rlimit_nofile 130048;\nworker_shutdown_timeout 10s;\n\nevents {\n  multi_accept on;\n  use epoll;\n  worker_connections 16384;\n}\n\nstream {\n  upstream kube_apiserver {\n    least_conn;\n    server ${MASTER_IPS[0]}:6443;\n    server ${MASTER_IPS[1]}:6443;\n    ...\n    server ${MASTER_IPS[N]}:6443;\n  }\n\n  server {\n    listen        127.0.0.1:6443;\n    proxy_pass    kube_apiserver;\n    proxy_timeout 10m;\n    proxy_connect_timeout 1s;\n  }\n}\n\nhttp {\n  aio threads;\n  aio_write on;\n  tcp_nopush on;\n  tcp_nodelay on;\n\n  keepalive_timeout 5m;\n  keepalive_requests 100;\n  reset_timedout_connection on;\n  server_tokens off;\n  autoindex off;\n\n  server {\n    listen 8081;\n    location /healthz {\n      access_log off;\n      return 200;\n    }\n    location /stub_status {\n      stub_status on;\n      access_log off;\n    }\n  }\n}\nEOF\n\nNginx manifest\n\ncat &lt;&lt;EOF &gt; /etc/kubernetes/manifests/nginx-proxy.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-proxy\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    k8s-app: kube-nginx\nspec:\n  hostNetwork: true\n  dnsPolicy: ClusterFirstWithHostNet\n  nodeSelector:\n    kubernetes.io/os: linux\n  priorityClassName: system-node-critical\n  containers:\n  - name: nginx-proxy\n    image: docker.io/library/nginx:1.19\n    imagePullPolicy: IfNotPresent\n    resources:\n      requests:\n        cpu: 25m\n        memory: 32M\n    securityContext:\n      privileged: true\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8081\n    readinessProbe:\n      httpGet:\n        path: /healthz\n        port: 8081\n    volumeMounts:\n    - mountPath: /etc/nginx\n      name: etc-nginx\n      readOnly: true\n  volumes:\n  - name: etc-nginx\n    hostPath:\n      path: /etc/nginx\nEOF\n\n配置kube-proxy\n\n\n配置文件\n\nmv kube-proxy.kubeconfig /etc/kubernetes/\ncat &lt;&lt;EOF &gt; /etc/kubernetes/kube-proxy-config.yaml\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nbindAddress: 0.0.0.0\nclientConnection:\n  kubeconfig: \"/etc/kubernetes/kube-proxy.kubeconfig\"\nclusterCIDR: \"10.200.0.0/16\"\nmode: ipvs\nEOF\n\nkube-proxy服务文件\n\ncat &lt;&lt;EOF &gt; /etc/systemd/system/kube-proxy.service\n[Unit]\nDescription=Kubernetes Kube Proxy\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kube-proxy \\\\\n  --config=/etc/kubernetes/kube-proxy-config.yaml\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n服务启动\n\nsystemctl daemon-reload\nsystemctl enable kubelet kube-proxy\nsystemctl start kubelet kube-proxy\njournalctl -f -u kubelet\njournalctl -f -u kube-proxy\n\n手动下载镜像pause(服务器无法访问外网) 在每个节点下载(master节点不下载可能不能成功)\n\ncrictl pull registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2\nctr -n k8s.io i tag  registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2 k8s.gcr.io/pause:3.2\n\n主动拉取/etc/kubernetes/manifests/nginx-proxy.yaml文件中的nginx镜像 具体版本，查看文件中的版本信息\n\ncrictl pull docker.io/library/nginx:1.19\n\n\n网络插件Calico\n\nYAML文件需要从官网下载\n\n\n链接: https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises\n有两种配置，50节点以内或者以上\n具体版本看官网信息，一下链接可能版本会不同\n\ncurl https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/calico.yaml -O\n\n修改IP自动发现方式\n\n\nautodetect 可能会有问题\n当kubelet的启动参数中存在–node-ip的时候，以host-network模式启动的pod的status.hostIP字段就会自动填入kubelet中指定的ip地址\n\n修改前:\n- name: IP\n  value: \"autodetect\"\n修改后:\n- name: IP\n  valueFrom:\n    fieldRef:\n      fieldPath: status.hostIP\n\n修改CIDR\n\n修改前:\n# - name: CALICO_IPV4POOL_CIDR\n#   value: \"192.168.0.0/16\"\n修改后:\n- name: CALICO_IPV4POOL_CIDR\n  value: \"10.200.0.0/16\"\n\n启动calico\n\nkubectl apply -f calico.yaml\n\n\nDNS插件CoreDNS\n\nDeploy CoreDNS\n\n\n设置coredns的cluster-ip\n下载coredns.yaml\n替换cluster-ip\n创建coredns\n\nCOREDNS_CLUSTER_IP=10.233.0.10\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n      addonmanager.kubernetes.io/mode: EnsureExists\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n            lameduck 5s\n        }\n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n        forward . /etc/resolv.conf {\n          prefer_udp\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n    addonmanager.kubernetes.io/mode: Reconcile\n  name: system:coredns\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - endpoints\n      - services\n      - pods\n      - namespaces\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n    addonmanager.kubernetes.io/mode: EnsureExists\n  name: system:coredns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:coredns\nsubjects:\n  - kind: ServiceAccount\n    name: coredns\n    namespace: kube-system\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/name: \"coredns\"\n    addonmanager.kubernetes.io/mode: Reconcile\n  annotations:\n    prometheus.io/port: \"9153\"\n    prometheus.io/scrape: \"true\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: ${COREDNS_CLUSTER_IP}\n  ports:\n    - name: dns\n      port: 53\n      protocol: UDP\n    - name: dns-tcp\n      port: 53\n      protocol: TCP\n    - name: metrics\n      port: 9153\n      protocol: TCP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: \"coredns\"\n  namespace: kube-system\n  labels:\n    k8s-app: \"kube-dns\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: \"coredns\"\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 10%\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n      annotations:\n        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'\n    spec:\n      priorityClassName: system-cluster-critical\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: coredns\n      tolerations:\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - topologyKey: \"kubernetes.io/hostname\"\n            labelSelector:\n              matchLabels:\n                k8s-app: kube-dns\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: In\n                values:\n                - \"\"\n      containers:\n      - name: coredns\n        image: \"docker.io/coredns/coredns:1.6.7\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          # TODO: Set memory limits when we've profiled the container for large\n          # clusters, then set request = limit to keep this container in\n          # guaranteed class. Currently, this container falls into the\n          # \"burstable\" category so the kubelet doesn't backoff from restarting it.\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args: [ \"-conf\", \"/etc/coredns/Corefile\" ]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - all\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8181\n            scheme: HTTP\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 10\n      dnsPolicy: Default\n      volumes:\n        - name: config-volume\n          configMap:\n            name: coredns\n            items:\n            - key: Corefile\n              path: Corefile\n  ```\n\n```bash\nsed -i \"s/\\${COREDNS_CLUSTER_IP}/${COREDNS_CLUSTER_IP}/g\" coredns.yaml\nkubectl apply -f coredns.yaml\n\nDeploy NodeLocal DNSCache\n\n\n设置coredns的cluster-ip\n下载nodelocaldns.yaml\n替换`cluster-ip\n创建nodelocaldns\n\nCOREDNS_CLUSTER_IP=10.233.0.10\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nodelocaldns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: EnsureExists\n\ndata:\n  Corefile: |\n    cluster.local:53 {\n        errors\n        cache {\n            success 9984 30\n            denial 9984 5\n        }\n        reload\n        loop\n        bind 169.254.25.10\n        forward . ${COREDNS_CLUSTER_IP} {\n            force_tcp\n        }\n        prometheus :9253\n        health 169.254.25.10:9254\n    }\n    in-addr.arpa:53 {\n        errors\n        cache 30\n        reload\n        loop\n        bind 169.254.25.10\n        forward . ${COREDNS_CLUSTER_IP} {\n            force_tcp\n        }\n        prometheus :9253\n    }\n    ip6.arpa:53 {\n        errors\n        cache 30\n        reload\n        loop\n        bind 169.254.25.10\n        forward . ${COREDNS_CLUSTER_IP} {\n            force_tcp\n        }\n        prometheus :9253\n    }\n    .:53 {\n        errors\n        cache 30\n        reload\n        loop\n        bind 169.254.25.10\n        forward . /etc/resolv.conf\n        prometheus :9253\n    }\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nodelocaldns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  selector:\n    matchLabels:\n      k8s-app: nodelocaldns\n  template:\n    metadata:\n      labels:\n        k8s-app: nodelocaldns\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9253'\n    spec:\n      priorityClassName: system-cluster-critical\n      serviceAccountName: nodelocaldns\n      hostNetwork: true\n      dnsPolicy: Default  # Don't use cluster DNS.\n      tolerations:\n      - effect: NoSchedule\n        operator: \"Exists\"\n      - effect: NoExecute\n        operator: \"Exists\"\n      containers:\n      - name: node-cache\n        image: \"registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/dns_k8s-dns-node-cache:1.16.0\"\n        resources:\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args: [ \"-localip\", \"169.254.25.10\", \"-conf\", \"/etc/coredns/Corefile\", \"-upstreamsvc\", \"coredns\" ]\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9253\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 169.254.25.10\n            path: /health\n            port: 9254\n            scheme: HTTP\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 10\n        readinessProbe:\n          httpGet:\n            host: 169.254.25.10\n            path: /health\n            port: 9254\n            scheme: HTTP\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n      volumes:\n        - name: config-volume\n          configMap:\n            name: nodelocaldns\n            items:\n            - key: Corefile\n              path: Corefile\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 20%\n    type: RollingUpdate\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nodelocaldns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\nsed -i \"s/\\${COREDNS_CLUSTER_IP}/${COREDNS_CLUSTER_IP}/g\" nodelocaldns.yaml\nkubectl apply -f nodelocaldns.yaml\n\n\n集群冒烟测试\n\n创建nginx ds\n\ncat &gt; nginx-ds.yml &lt;&lt;EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-ds\n  labels:\n    app: nginx-ds\nspec:\n  type: NodePort\n  selector:\n    app: nginx-ds\n  ports:\n  - name: http\n    port: 80\n    targetPort: 80\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-ds\nspec:\n  selector:\n    matchLabels:\n      app: nginx-ds\n  template:\n    metadata:\n      labels:\n        app: nginx-ds\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx:1.19\n        ports:\n        - containerPort: 80\nEOF\n\nkubectl apply -f nginx-ds.yml\n\n检查各种IP连通性\n\n\n查看各Node上的Pod IP连通性\n\nkubectl get pods -o wide\n如下图 \n\n在每个Worker节点上ping pod ip\n\nping &lt;pod-ip&gt; #上图中查出来的\n\n查看service可达性\n\nkubectl get svc\n\n# 在每个`Worker`节点上访问服务\ncurl &lt;service-ip&gt;:&lt;port&gt; # 80\n\n在每个节点上检查`node-port`可用性\ncurl &lt;node-ip&gt;:&lt;port&gt; # node-ip: 192.168.200.22, port: 上图中80后面的端口号\n如下图: \n\n检查dns可用性\n\ncat &gt; pod-nginx.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: docker.io/library/nginx:1.19\n    ports:\n    - containerPort: 80\nEOF\n\n# 创建pod\n$ kubectl apply -f pod-nginx.yaml\n\n# 进入pod，查看dns(根据pod名)\n$ kubectl exec nginx -it -- /bin/bash\n\n# 查看dns配置(IP)\nroot@nginx:/# cat /etc/resolv.conf\n\n# 查看名字是否可以正确解析\nroot@nginx:/# curl nginx-ds"
  },
  {
    "objectID": "posts/poetry_pyright_doom_emacs/index.html",
    "href": "posts/poetry_pyright_doom_emacs/index.html",
    "title": "Doom Emacs & Poetry & Pyright管理Python开发环境",
    "section": "",
    "text": "Doom Emacs的python配置\n\n(python\n  +poetry\n  +pyright)\n\nInstall Poetry\n\npip install poetry\n\nInstall Pyright\n\nnpm install -g pyright\n\n\n\nmkdir project_name && cd project_name\npoetry init\npoetry install\npoetry add flask\nEmacs打开project_name/app.py\n这样Doom Emacs配置的Poetry的虚拟环境中的Flask,可以在python的LSP补全后端补全flask的代码了\n\n\n\n刚配置完成后，创建Poetry项目后可以正常激活，但是Pyright始终无法补全 最终在Reddit Link 中有人编辑器用了Github Repo上的一个项目后，然后 打开别的Poetry创建的项目，有人讨论说，可能是因为上面的Github仓库中多了poetry.toml文件 内容如下:\n[virtualenvs]\nin-project = true"
  },
  {
    "objectID": "posts/poetry_pyright_doom_emacs/index.html#记录doom-emacs-使用poetry管理的多虚拟环境使用pyright补全",
    "href": "posts/poetry_pyright_doom_emacs/index.html#记录doom-emacs-使用poetry管理的多虚拟环境使用pyright补全",
    "title": "Doom Emacs & Poetry & Pyright管理Python开发环境",
    "section": "",
    "text": "Doom Emacs的python配置\n\n(python\n  +poetry\n  +pyright)\n\nInstall Poetry\n\npip install poetry\n\nInstall Pyright\n\nnpm install -g pyright\n\n\n\nmkdir project_name && cd project_name\npoetry init\npoetry install\npoetry add flask\nEmacs打开project_name/app.py\n这样Doom Emacs配置的Poetry的虚拟环境中的Flask,可以在python的LSP补全后端补全flask的代码了\n\n\n\n刚配置完成后，创建Poetry项目后可以正常激活，但是Pyright始终无法补全 最终在Reddit Link 中有人编辑器用了Github Repo上的一个项目后，然后 打开别的Poetry创建的项目，有人讨论说，可能是因为上面的Github仓库中多了poetry.toml文件 内容如下:\n[virtualenvs]\nin-project = true"
  },
  {
    "objectID": "posts/started/index.html",
    "href": "posts/started/index.html",
    "title": "你好, Quarto",
    "section": "",
    "text": "今天把博客从Hugo迁移到Quarto。在这一天也有了自己人生未来的新计划。\n                             | |\n__  ___   _  __ _ _ __    ___| |__  _   _\n\\ \\/ / | | |/ _` | '_ \\  / __| '_ \\| | | |\n &gt;  &lt;| |_| | (_| | | | | \\__ \\ | | | |_| |\n/_/\\_\\\\__,_|\\__,_|_| |_| |___/_| |_|\\__,_|\n__   __    __   ___\n\\ \\ / /    \\ \\ / (_)\n \\ V /___   \\ V / _ _ __   __ _\n  \\ // _ \\   \\ / | | '_ \\ / _` |\n  | |  __/   | | | | | | | (_| |\n  \\_/\\___|   \\_/ |_|_| |_|\\__, |\n                           __/ |\n                          |___/"
  },
  {
    "objectID": "posts/started/index.html#新的开始",
    "href": "posts/started/index.html#新的开始",
    "title": "你好, Quarto",
    "section": "",
    "text": "今天把博客从Hugo迁移到Quarto。在这一天也有了自己人生未来的新计划。\n                             | |\n__  ___   _  __ _ _ __    ___| |__  _   _\n\\ \\/ / | | |/ _` | '_ \\  / __| '_ \\| | | |\n &gt;  &lt;| |_| | (_| | | | | \\__ \\ | | | |_| |\n/_/\\_\\\\__,_|\\__,_|_| |_| |___/_| |_|\\__,_|\n__   __    __   ___\n\\ \\ / /    \\ \\ / (_)\n \\ V /___   \\ V / _ _ __   __ _\n  \\ // _ \\   \\ / | | '_ \\ / _` |\n  | |  __/   | | | | | | | (_| |\n  \\_/\\___|   \\_/ |_|_| |_|\\__, |\n                           __/ |\n                          |___/"
  }
]